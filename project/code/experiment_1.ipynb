{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sklearn.datasets as skds\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 347)\n",
      "{u'gr\\xf6\\xdfe': 148, u'rendir': 265, u'charles': 62, u'ansehen': 24, u'germar': 140, u'bereits': 51, u'southwestern': 297, u'compararlo': 69, u'religious': 263, u'jahre': 168, u'paris': 239, u'state': 300, u'to': 310, u'aus': 40, u'hermann': 155, u'hatte': 152, u'apos': 26, u'l\\xe4nge': 196, u'beau': 44, u'de': 80, u'lendenwirbel': 189, u'vuelven': 326, u'algunos': 18, u'\\xe9diteur': 343, u'llama': 192, u'during': 97, u'morceaux': 210, u'qu': 255, u'becken': 47, u'die': 85, u'ausgestellt': 41, u'vingt': 323, u'dixi\\xe8me': 93, u'qotsa': 254, u'kasaragod': 174, u'm\\xfasica': 217, u'karnataka': 173, u'collectively': 66, u'are': 29, u'entretiens': 108, u'districts': 92, u'weitere': 330, u'remporta': 264, u'coqu\\xe9au': 73, u'kupferschiefer': 179, u'sur': 305, u'rite': 271, u'guns': 149, u'publia': 253, u'pregunto': 247, u'latin': 187, u'diese': 86, u'royale': 276, u'r\\xe9pertoire': 277, u'abb\\xe9': 12, u'nicht': 224, u'\\xe9tait': 344, u'sind': 294, u'schleusingen': 286, u'aunque': 39, u'39': 8, u'funden': 136, u'region': 262, u'1840': 4, u'ahora': 17, u'equivocado': 109, u'prior': 249, u'roman': 274, u'und': 318, u'soup\\xe7onna': 295, u'entier': 107, u'ernst': 113, u'resta': 268, u'hallesche': 150, u'buscan': 57, u'sachsen': 281, u'con': 71, u'morellet': 211, u'beschrieb': 52, u'gottmann': 143, u'como': 68, u'exemplares': 126, u'cuando': 75, u'community': 67, u'qui': 258, u'unos': 320, u'64': 10, u'ante': 25, u'naturhistorischen': 220, u'tiene': 309, u'from': 133, u'friedrich': 132, u'wien': 334, u'pero': 241, u'mort': 212, u'estaban': 119, u'cuarto': 76, u'eran': 112, u'dont': 94, u'suard': 303, u'zweit\\xe4ltesten': 341, u'is': 165, u'trabajo': 312, u'implica': 160, u'wurde': 338, u'demasiado': 82, u'fund': 134, u'fest': 127, u'griffe': 145, u'heute': 156, u'brochure': 56, u'kannada': 172, u'quatre': 256, u'me': 203, u'katholik': 175, u'ethno': 124, u'this': 307, u'asegura': 32, u'der': 84, u'naturkunde': 221, u'repr\\xe9sentation': 267, u'westf\\xe4lischen': 333, u'dem': 81, u'den': 83, u'weiteren': 331, u'following': 129, u'ein': 98, u'bebidas': 46, u'noch': 227, u'consigue': 72, u'von': 324, u'acad\\xe9mie': 13, u'india': 162, u'it': 167, u'states': 301, u'waldenburg': 327, u'im': 159, u'british': 55, u'vor': 325, u'in': 161, u'una': 317, u'il': 158, u'cr\\xedtica': 74, u'located': 194, u'grand': 144, u'tout': 311, u'a\\xf1os': 42, u'une': 319, u'nirvana': 225, u'sch\\xe4tzungsweise': 288, u'buscando': 58, u'coast': 65, u'estado': 120, u'laquelle': 185, u'protorosaurus': 251, u'wirbelknochen': 336, u'm\\xed': 216, u'atreven': 35, u'equivocando': 110, u'pr\\xe4sakralwirbel': 252, u'si': 291, u'kurz': 182, u'sa': 280, u'se': 289, u'bien': 53, u'atenci\\xf3n': 34, u'gro\\xdfen': 146, u'meter': 205, u'1787': 1, u'nuestra': 229, u'canara': 59, u'malgr\\xe9': 197, u'insgesamt': 163, u'est\\xe1': 121, u'26': 7, u'le': 188, u'la': 183, u'lo': 193, u'gente': 139, u'1839': 3, u'bisher': 54, u'kerala': 176, u'reunir': 269, u'signific\\xf3': 293, u'as\\xed': 33, u'spectateur': 298, u'ensalzan': 106, u'fragmentarischen': 131, u'maximal': 202, u'aqu\\xed': 27, u'was': 329, u'lambert': 184, u'por': 246, u'succ\\xe8s': 304, u'fiesta': 128, u'1956': 5, u'ces': 61, u'durante': 96, u'belegt': 50, u'beaux': 45, u'dormitorio': 95, u'philibert': 242, u'nevermind': 223, u'linck': 191, u'th\\xfcringen': 308, u'50': 9, u'man': 198, u'rock': 272, u'schwanzwirbel': 287, u'referred': 260, u'kann': 171, u'el': 103, u'en': 105, u'dakshina': 77, u'diocese': 89, u'salven': 284, u'fossilien': 130, u'halswirbel': 151, u'comprises': 70, u'mangalorean': 200, u'ahogando': 16, u'et': 123, u'es': 116, u'er': 111, u'est': 118, u'1717': 0, u'all\\xe1': 19, u'zwei': 340, u'trouvait': 313, u'\\u0153uvre': 346, u'nomm\\xe9': 228, u'vertement': 322, u'dieser': 87, u'morceau': 209, u'eso': 117, u'zuvor': 339, u'parisienne': 240, u'o\\xf9': 236, u'verdadero': 321, u'auch': 38, u'nuevo': 230, u'que': 257, u'gefunden': 138, u'meyer': 207, u'on': 233, u'gluck': 142, u'jeune': 169, u'arnaud': 30, u'of': 232, u'wovon': 337, u'ist': 166, u'marmontel': 201, u'act': 14, u'wei\\xdf': 332, u'podemos': 243, u'south': 296, u'naturalienkabinett': 219, u'heavy': 154, u'eines': 101, u'civil': 63, u'monographie': 208, u'claude': 64, u'pop': 245, u'musique': 215, u'eine': 99, u'einen': 100, u'f\\xfcr': 137, u'gesch\\xe4tzt': 141, u'sander': 285, u'he': 153, u'rated': 259, u'panckoucke': 237, u'district': 91, u'salieron': 283, u'est\\xe1n': 122, u'museum': 214, u'revive': 270, u'los': 195, u'liegt': 190, u'joseph': 170, u'prensa': 248, u'saint': 282, u'sich': 292, u'las': 186, u'mercure': 204, u'war': 328, u'elementos': 104, u'catholics': 60, u'r\\xe9pliqua': 278, u'stellte': 302, u'wimmelburg': 335, u'para': 238, u'r\\xe9plique': 279, u'intitul\\xe9e': 164, u'tr\\xe8s': 314, u'erstwhile': 115, u'udupi': 315, u'op\\xe9ra': 234, u'\\xe4hnliches': 342, u'funde': 135, u'distintos': 90, u'bei': 48, u'18': 2, u'dans': 78, u'm\\xfcnster': 218, u'un': 316, u'exemplar': 125, u'mangalore': 199, u'and': 23, u'eisleben': 102, u'dass': 79, u'am': 20, u'necesita': 222, u'an': 22, u'as': 31, u'au': 37, u'reorganisation': 266, u'erreichbare': 114, u'seiner': 290, u'regime': 261, u'organizando': 235, u'no': 226, u'dijeron': 88, u'attaqu\\xe9': 36, u'roses': 275, u'kupfersuhl': 180, u'kupfersuhler': 181, u'konkani': 178, u'homme': 157, u'roll': 273, u'ba\\xf1o': 43, u'nuevos': 231, u'belegst\\xfcck': 49, u'\\xe9tat': 345, u'70': 11, u'architecte': 28, u'grupo': 147, u'the': 306, u'kodialchein': 177, u'stammt': 299, u'amputa': 21, u'professor': 250, u'muchos': 213, u'actuel': 15, u'pol\\xe9miques': 244, u'2009': 6, u'meternos': 206}\n"
     ]
    }
   ],
   "source": [
    "# First tokenization\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_counts = count_vectorizer.fit_transform(ds.data)\n",
    "\n",
    "print X_train_counts.shape\n",
    "print count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 347)\n"
     ]
    }
   ],
   "source": [
    "# Term Frequency Transformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "print X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1]\n",
      "  (0, 29)\t1.0\n",
      "  (1, 71)\t0.707106781187\n",
      "  (1, 203)\t0.707106781187\n",
      "'Hello, how are you?' => english\n",
      "'Yo me guesta juegar con sus bano' => spanish\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes classification\n",
    "# Trained the classifier on our documents\n",
    "clf = MultinomialNB().fit(X_train_tf, ds.target)\n",
    "\n",
    "docs_new = [\"Hello, how are you?\", \"Yo me guesta juegar con sus bano\"]\n",
    "docs_counts = count_vectorizer.transform(docs_new)\n",
    "docs_tf = tf_transformer.transform(docs_counts)\n",
    "print docs_counts.data\n",
    "print docs_tf\n",
    "\n",
    "\n",
    "# Display predictions\n",
    "predicted = clf.predict(docs_tf)\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print ('%r => %s' % (doc, ds.target_names[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**load_corpus**\n",
    "Returns the experimental corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_corpus(source=\"../data/experimental_corpus\", encoding='UTF-8'):\n",
    "    return skds.load_files(source, encoding=encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dev_corpus():\n",
    "    return load_corpus(source='../data/dev_corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**alpha_only_token_pattern**\n",
    "\n",
    "Returns a token pattern for unicode alphabetical character words only.  No numbers or underscores allowed.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def alpha_only_token_pattern():\n",
    "    return '(?u)\\\\b[^\\W_0-9]{1,}\\\\b'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Experiment 1:  Naive Bayes using Feature Counts only_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word features are vectorized per:\n",
    "- Alpha only tokens, single characters allowed\n",
    "- binary representation only (token exists or does not)\n",
    "- all lowercase\n",
    "- UTF-8 encoding\n",
    "- no stopwords used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400 samples\n",
      "Category 0 is de\n",
      "Category 1 is en\n",
      "Category 2 is es\n",
      "Category 3 is fr\n",
      "The shape of the vocabulary: 400\n",
      "17272 features were found in the corpus.\n",
      "Feature 0 is a. length: 1\n",
      "Feature 1 is aa. length: 2\n",
      "Feature 2 is aaaaa. length: 5\n",
      "Feature 3 is aachen. length: 6\n",
      "Feature 4 is ab. length: 2\n",
      "Feature 5 is abaissée. length: 8\n",
      "Feature 6 is abajo. length: 5\n",
      "Feature 7 is abandon. length: 7\n",
      "Feature 8 is abandona. length: 8\n",
      "Feature 9 is abandonado. length: 10\n",
      "Feature 10 is abandoned. length: 9\n",
      "Feature 11 is abandonne. length: 9\n",
      "Feature 12 is abandonner. length: 10\n",
      "Feature 13 is abandonnera. length: 11\n",
      "Feature 14 is abandonné. length: 9\n",
      "Feature 15 is abandonó. length: 8\n",
      "Feature 16 is abattant. length: 8\n",
      "Feature 17 is abattue. length: 7\n",
      "Feature 18 is abbakka. length: 7\n",
      "Feature 19 is abbaye. length: 6\n",
      "Towards is 15251\n",
      "First training sample is:\n",
      "A large part of the C++ library is based on the STL. This provides useful tools as containers (for example vectors and lists), iterators to provide these containers with array-like access and algorithms to perform operations such as searching and sorting. Furthermore (multi)maps (associative arrays) and (multi)sets are provided, all of which export compatible interfaces. Therefore it is possible, using templates, to write generic algorithms that work with any container or on any sequence defined by iterators. As in C, the features of the library are accessed by using the  #include  directive to include a standard header. C++ provides 105 standard headers, of which 27 are deprecated.\n",
      "\n",
      "Feature vector is:\n",
      "[[1 0 0 ..., 0 0 0]]\n",
      "Checking for presence of the. Indicatoris 1\n",
      "Checking for presence of acquis. Indicator is 0\n",
      "set([])\n"
     ]
    }
   ],
   "source": [
    "corpus = load_dev_corpus()\n",
    "\n",
    "print \"Loaded {0} samples\".format(len(corpus.data))\n",
    "\n",
    "# Classification labels\n",
    "for index, name in enumerate(corpus.target_names):\n",
    "    print \"Category {0} is {1}\".format(index, name)\n",
    "\n",
    "    \n",
    "# Starting with a simple Binary Word Tokenizer,  requiring tokens at least 2 letters in length \n",
    "# eliminate numerical features\n",
    "cv = CountVectorizer(analyzer='word',binary=True, lowercase=True, encoding='UTF-8', \n",
    "                     token_pattern=alpha_only_token_pattern())\n",
    "voc = cv.fit_transform(corpus.data)\n",
    "print \"The shape of the vocabulary: {0}\".format(voc.shape[0])\n",
    "\n",
    "\n",
    "# Print the entire set of feature names\n",
    "print \"{0} features were found in the corpus.\".format(len(cv.get_feature_names()))\n",
    "for i, name in enumerate(cv.get_feature_names()[0:20]):\n",
    "    print u\"Feature {0} is {1}. length: {2}\".format(i, name, len(name))\n",
    "\n",
    "    \n",
    "# Check for numbers\n",
    "numbers = re.compile('\\d')\n",
    "for feature in cv.get_feature_names():\n",
    "    if numbers.search(feature):\n",
    "        print u\"Feature {0} has numbers in it.\".format(feature)\n",
    "\n",
    "print \"Towards is {0}\".format(cv.vocabulary_['towards'])\n",
    "print u\"First training sample is:\\n{0}\".format(corpus.data[0])\n",
    "print \"Feature vector is:\\n{0}\".format(voc[0].toarray())    \n",
    "\n",
    "fet1 = cv.vocabulary_['the']\n",
    "print u'Checking for presence of {0}. Indicatoris {1}'.format('the', voc[0].toarray()[0][fet1])\n",
    "fet2 = cv.vocabulary_['acquis']\n",
    "print u'Checking for presence of {0}. Indicator is {1}'.format('acquis', voc[0].toarray()[0][fet2])\n",
    "\n",
    "print cv.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target name: de, id: 1\n",
      "Target name: en, id: 3\n",
      "Target name: es, id: 3\n",
      "Target name: fr, id: 1\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "for name, ident in zip(corpus.target_names, corpus.target):\n",
    "    print \"Target name: {0}, id: {1}\".format(name, ident)\n",
    "    \n",
    "print len(corpus.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Try splits\n",
    "train_corpus, test_corpus, train_targets, test_targets = train_test_split(corpus.data, corpus.target, test_size=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test w/ cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you? => en\n",
      "Yo me guesta juegar con sus bano => es\n",
      "hamburger on wheat with peanuts => en\n",
      "[ 1.      0.9875  1.      1.      1.    ]\n"
     ]
    }
   ],
   "source": [
    "# Load dev corpus\n",
    "dev_corpus = load_dev_corpus()\n",
    "\n",
    "# Create our vectorizer for the corpus\n",
    "vectorizer = CountVectorizer(analyzer='word', binary=True, lowercase=True, encoding='UTF-8', \n",
    "                            token_pattern=alpha_only_token_pattern())\n",
    "exp_data = vectorizer.fit_transform(corpus.data)\n",
    "\n",
    "# Train a Naive Bayes classifier on our vectorized corpus\n",
    "classifier = MultinomialNB().fit(exp_data, dev_corpus.target)\n",
    "\n",
    "# Simple classification test\n",
    "docs_new = [\"Hello, how are you?\", \"Yo me guesta juegar con sus bano\", 'hamburger on wheat with peanuts']\n",
    "docs_counts = vectorizer.transform(docs_new)\n",
    "predicted = classifier.predict(docs_counts)\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print '{0} => {1}'.format(doc, dev_corpus.target_names[category])\n",
    "    \n",
    "# Cross validation\n",
    "X = exp_data\n",
    "y = dev_corpus.target\n",
    "scores = cross_val_score(classifier, X, y, cv=5)\n",
    "print scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
